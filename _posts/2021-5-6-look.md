---
published: true
title: Adaptative Boosting (AdaBoost)
collection: st
layout: single
author_profile: true
read_time: true
categories: [machinelearning]
excerpt : "Supervised Learning Algorithms"
header :
    overlay_image: "https://maelfabien.github.io/assets/images/wolf.jpg"
    teaser: "https://maelfabien.github.io/assets/images/wolf.jpg"
comments : true
toc: true
toc_sticky: true
sidebar:
    nav: sidebar-sample
---
Boosting techniques have recently been rising in Kaggle competitions and other predictive analysis tasks. You may have heard of them under the names of XGBoost or LGBM. In this tutorial, we'll go through Adaboost, one of the first boosting techniques discovered. 

This article can also be found on <a href="https://towardsdatascience.com/boosting-and-adaboost-clearly-explained-856e21152d3e">Towards Data Science</a>.

<script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

## The limits of Bagging
